{
  "Header": {
    "tooltipSettings": "Einstellungen",
    "tooltipTheme": "Design",
    "tooltipLanguage": "Sprache",
    "manualSettings": "Manuelle Einstellungen"
  },
  "ConversationList": {
    "Conversations": "Konversationen",
    "sidebarClose": "Seitenleiste schließen",
    "newConversation": "Neue Konversation",
    "convInformation": "Konversationen werden in der IndexedDB des Browsers gespeichert",
    "deleteConfirm": "Möchten Sie diese Konversation wirklich löschen?",
    "downloadBtn": "Herunterladen",
    "deleteBtn": "Löschen",
    "conversationBtn": "Konversationen",
    "closeBtn": "Schließen"
  },
  "ChatScreen": {
    "suggestions": "Hier sind einige Vorschläge für Sie:",
    "sendMsgStart": "Senden Sie eine Nachricht, um zu beginnen",
    "textAreaPlaceHolder": "Geben Sie eine Nachricht ein (Umschalt+Eingabe, um eine neue Zeile hinzuzufügen)",
    "stopBtn": "Stopp",
    "sendBtn": "Senden"
  },
  "ChatMessage": {
    "Time": "Zeit",
    "Speed": "Geschwindigkeit",
    "Tokens": "Tokens",
    "Cancel": "Abbrechen",
    "Submit": "Senden",
    "Edit": "✍\uFE0F Bearbeiten",
    "Regenerate": "\uD83D\uDD04 Regenerieren",
    "Thinking": "Denken",
    "ThoughtProcess": "Gedankenprozess",
    "ExtraContent": "Zusätzlicher Inhalt"
  },
  "MarkdownDisplay": {
    "copied": "Kopiert!",
    "copy": "\uD83D\uDCCB Kopieren",
    "Run": "Ausführen",
    "PythonInterpreter": "Python-Interpreter"
  },
  "Settings": {
    "Settings": "Einstellungen",
    "CloseBtn": "Schließen",
    "savedLocal": "Einstellungen werden im lokalen Speicher des Browsers gespeichert",
    "resetConfirm": "Möchten Sie wirklich alle Einstellungen zurücksetzen?",
    "resetBtn": "Auf Standard zurücksetzen",
    "saveBtn": "Speichern",
    "loadPresetBtn": "JSON-Datei mit Voreinstellungen laden",
    "savePresetBtn": "JSON-Datei mit Voreinstellungen speichern",
    "presetLabel": "Voreinstellungen",
    "tooltipPresets": "Voreinstellungen",
    "sections": {
      "General": "Allgemein",
      "Samplers": "Sampler",
      "Penalties": "Strafen",
      "Reasoning": "Begründung",
      "Advanced": "Erweitert",
      "Experimental": "Experimentell"
    },
    "labels": {
      "apiKey": "API-Schlüssel",
      "systemMessage": "Systemnachricht (wird deaktiviert, wenn leer gelassen)",
      "samplers": "Sampler-Warteschlange",
      "temperature": "",
      "dynatemp_range": "",
      "dynatemp_exponent": "",
      "top_k": "",
      "top_p": "",
      "min_p": "",
      "xtc_probability": "",
      "xtc_threshold": "",
      "typical_p": "",
      "repeat_last_n": "",
      "repeat_penalty": "",
      "presence_penalty": "",
      "frequency_penalty": "",
      "dry_multiplier": "",
      "dry_base": "",
      "dry_allowed_length": "",
      "dry_penalty_last_n": "",
      "max_tokens": "",
      "customBtn": "(debug) Demo-Konversation importieren",
      "showThoughtInProgress": "Standardmäßig durch Prozess erweitern, um Nachricht zu generieren",
      "excludeThoughtOnReq": "Denkprozess beim Senden von Anfragen an die API ausschließen (empfohlen für DeepSeek-R1)",
      "showTokensPerSecond": "Token pro Sekunde anzeigen",
      "custom": "Benutzerdefinierte JSON-Konfiguration (Weitere Informationen finden Sie in der",
      "customLinkLabel": "Serverdokumentation",
      "Experimental1": "Es wird nicht garantiert, dass experimentelle Funktionen korrekt funktionieren.",
      "Experimental2": "Wenn Sie auf Probleme stoßen, erstellen Sie einen",
      "Experimental3": "Bericht auf Github. Bitte geben Sie im Berichtstitel auch <b>webui/experimental</b> an und fügen Sie Screenshots bei.",
      "Experimental4": "Für einige Funktionen müssen möglicherweise Pakete vom CDN heruntergeladen werden, daher ist eine Internetverbindung erforderlich.",
      "pyIntepreter1": "Python-Interpreter aktivieren",
      "pyIntepreter2": "Diese Funktion verwendet",
      "pyIntepreter3": "vom CDN heruntergeladen. Um diese Funktion zu verwenden, bitten Sie das LLM, Python-Code innerhalb eines Markdown-Codeblocks zu generieren. Sie sehen im Codeblock neben der Schaltfläche \"Kopieren\", eine Schaltfläche \"Ausführen\"",
      ",handleSave1": ",Wert für",
      ",handleSave2": ",muss eine Zeichenfolge sein",
      ",handleSave3": ",muss eine Zahl sein",
      ",handleSave4": ",muss ein Boolescher Wert sein",
      ",handleSave5": ",muss ein Array sein"
    },
    "meaning": {
      "apiKey": "Legen Sie den API-Schlüssel fest, wenn Sie die Option --api-key für den Server verwenden.",
      "systemMessage": "Die Startnachricht, die definiert, wie sich das Modell verhalten soll.",
      "samplers": "Die Reihenfolge, in der Sampler angewendet werden, vereinfacht dargestellt. Standard ist 'dkypmxt': dry->top_k->typ_p->top_p->min_p->xtc->temperature",
      "temperature": "Steuert die Zufälligkeit des generierten Textes, indem die Wahrscheinlichkeitsverteilung der Ausgabetoken beeinflusst wird. Höher = zufälliger, niedriger = fokussierter.",
      "dynatemp_range": "Addon für den Temperatursampler. Der hinzugefügte Wert zum Bereich der dynamischen Temperatur, der die Wahrscheinlichkeiten durch die Entropie der Token anpasst.",
      "dynatemp_exponent": "Addon für den Temperatursampler. Glättet die Wahrscheinlichkeitsumverteilung basierend auf dem wahrscheinlichsten Token.",
      "top_k": "Behält nur k Top-Token.",
      "top_p": "Beschränkt Token auf diejenigen, die zusammen eine kumulative Wahrscheinlichkeit von mindestens p haben",
      "min_p": "Beschränkt Token basierend auf der Mindestwahrscheinlichkeit, mit der ein Token berücksichtigt wird, relativ zur Wahrscheinlichkeit des wahrscheinlichsten Tokens.",
      "xtc_probability": "XTC-Sampler schneidet Top-Token aus; dieser Parameter steuert die Wahrscheinlichkeit, überhaupt Token auszuschneiden. 0 deaktiviert XTC.",
      "xtc_threshold": "XTC-Sampler schneidet Top-Token aus; dieser Parameter steuert die Tokenwahrscheinlichkeit, die zum Ausschneiden dieses Tokens erforderlich ist.",
      "typical_p": "Sortiert und begrenzt Token basierend auf der Differenz zwischen Log-Wahrscheinlichkeit und Entropie.",
      "repeat_last_n": "Letzte n Token, die zur Bestrafung von Wiederholungen berücksichtigt werden sollen",
      "repeat_penalty": "Steuert die Wiederholung von Tokensequenzen im generierten Text",
      "presence_penalty": "Begrenzt Token basierend darauf, ob sie in der Ausgabe erscheinen oder nicht.",
      "frequency_penalty": "Begrenzt Token basierend darauf, wie oft sie in der Ausgabe erscheinen.",
      "dry_multiplier": "DRY-Sampling reduziert Wiederholungen im generierten Text sogar über lange Kontexte hinweg. Dieser Parameter legt den DRY-Sampling-Multiplikator fest.",
      "dry_base": "DRY-Sampling reduziert Wiederholungen im generierten Text sogar über lange Kontexte hinweg. Dieser Parameter legt den DRY-Sampling-Basiswert fest.",
      "dry_allowed_length": "DRY-Sampling reduziert Wiederholungen im generierten Text sogar über lange Kontexte hinweg. Dieser Parameter legt die zulässige Länge für DRY-Sampling fest.",
      "dry_penalty_last_n": "DRY-Sampling reduziert Wiederholungen im generierten Text auch in langen Kontexten. Dieser Parameter legt die DRY-Strafe für die letzten n Token fest.",
      "max_tokens": "Die maximale Anzahl von Token pro Ausgabe."
    }
  }
}
