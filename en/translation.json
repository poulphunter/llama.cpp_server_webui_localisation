{
  "Header": {
    "tooltipSettings": "Settings",
    "tooltipTheme": "Theme",
    "tooltipLanguage": "Language",
    "manualSettings": "Manual settings"
  },
  "ConversationList": {
    "Conversations": "Conversations",
    "sidebarClose": "close sidebar",
    "newConversation": "New conversation",
    "convInformation": "Conversations are saved to browser's IndexedDB",
    "deleteConfirm": "Are you sure to delete this conversation?",
    "downloadBtn": "Download",
    "deleteBtn": "Delete",
    "conversationBtn": "Conversations",
    "closeBtn": "Close"
  },
  "ChatScreen": {
    "suggestions": "Here are some suggestions for you:",
    "sendMsgStart": "Send a message to start",
    "textAreaPlaceHolder": "Type a message (Shift+Enter to add a new line)",
    "stopBtn": "Stop",
    "sendBtn": "Send"
  },
  "ChatMessage": {
    "Time": "Time",
    "Speed": "Speed",
    "Tokens": "Tokens",
    "Cancel": "Cancel",
    "Submit": "Submit",
    "Edit": "‚úç\uFE0F Edit",
    "Regenerate": "\uD83D\uDD04 Regenerate",
    "Thinking": "Thinking",
    "ThoughtProcess": "Thought Process",
    "ExtraContent": "Extra content"
  },
  "MarkdownDisplay": {
    "copied": "Copied!",
    "copy": "\uD83D\uDCCB Copy",
    "Run": "Run",
    "PythonInterpreter": "Python Interpreter"
  },
  "Settings": {
    "Settings": "Settings",
    "CloseBtn": "Close",
    "savedLocal": "Settings are saved in browser's localStorage",
    "resetConfirm": "Are you sure to reset all settings?",
    "resetBtn": "Reset to default",
    "saveBtn": "Save",
    "loadPresetBtn": "Load Presets JSON file",
    "savePresetBtn": "Save Presets JSON file",
    "presetLabel": "Presets",
    "tooltipPresets": "Presets",
    "sections": {
      "General": "General",
      "Samplers": "Samplers",
      "Penalties": "Penalties",
      "Reasoning": "Reasoning",
      "Advanced": "Advanced",
      "Experimental": "Experimental"
    },
    "labels": {
      "apiKey": "API Key",
      "systemMessage": "System Message (will be disabled if left empty)",
      "samplers": "Samplers queue",
      "temperature": "",
      "dynatemp_range": "",
      "dynatemp_exponent": "",
      "top_k": "",
      "top_p": "",
      "min_p": "",
      "xtc_probability": "",
      "xtc_threshold": "",
      "typical_p": "",
      "repeat_last_n": "",
      "repeat_penalty": "",
      "presence_penalty": "",
      "frequency_penalty": "",
      "dry_multiplier": "",
      "dry_base": "",
      "dry_allowed_length": "",
      "dry_penalty_last_n": "",
      "max_tokens": "",
      "customBtn": "(debug) Import demo conversation",
      "showThoughtInProgress": "Expand though process by default for generating message",
      "excludeThoughtOnReq": "Exclude thought process when sending request to API (Recommended for DeepSeek-R1)",
      "showTokensPerSecond": "Show tokens per second",
      "custom": "Custom JSON config (For more info, refer to",
      "customLinkLabel": "server documentation",
      "Experimental1": "Experimental features are not guaranteed to work correctly.",
      "Experimental2": "If you encounter any problems, create a",
      "Experimental3": "report on Github. Please also specify <b>webui/experimental</b> on the report title and include screenshots.",
      "Experimental4": "Some features may require packages downloaded from CDN, so they need internet connection.",
      "pyIntepreter1": "Enable Python interpreter",
      "pyIntepreter2": "This feature uses",
      "pyIntepreter3": "downloaded from CDN. To use this feature, ask the LLM to generate python code inside a markdown code block. You will see a \"Run\" button on the code block, near the \"Copy\" button.",
      "handleSave1": "Value for",
      "handleSave2": "must be string",
      "handleSave3": "must be numeric",
      "handleSave4": "must be boolean",
      "handleSave5": "must be array"
    },
    "meaning": {
      "apiKey": "Set the API Key if you are using --api-key option for the server.",
      "systemMessage": "The starting message that defines how model should behave.",
      "samplers": "The order at which samplers are applied, in simplified way. Default is 'dkypmxt' : dry->top_k->typ_p->top_p->min_p->xtc->temperature",
      "temperature": "Controls the randomness of the generated text by affecting the probability distribution of the output tokens. Higher = more random, lower = more focused.",
      "dynatemp_range": "Addon for the temperature sampler. The added value to the range of dynamic temperature, which adjusts probabilities by entropy of tokens.",
      "dynatemp_exponent": "Addon for the temperature sampler. Smoothes out the probability redistribution based on the most probable token.",
      "top_k": "Keeps only k top tokens.",
      "top_p": "Limits tokens to those that together have a cumulative probability of at least p",
      "min_p": "Limits tokens based on the minimum probability for a token to be considered, relative to the probability of the most likely token.",
      "xtc_probability": "XTC sampler cuts out top tokens; this parameter controls the chance of cutting tokens at all. 0 disables XTC.",
      "xtc_threshold": "XTC sampler cuts out top tokens; this parameter controls the token probability that is required to cut that token.",
      "typical_p": "Sorts and limits tokens based on the difference between log-probability and entropy.",
      "repeat_last_n": "Last n tokens to consider for penalizing repetition",
      "repeat_penalty": "Controls the repetition of token sequences in the generated text",
      "presence_penalty": "Limits tokens based on whether they appear in the output or not.",
      "frequency_penalty": "Limits tokens based on how often they appear in the output.",
      "dry_multiplier": "DRY sampling reduces repetition in generated text even across long contexts. This parameter sets the DRY sampling multiplier.",
      "dry_base": "DRY sampling reduces repetition in generated text even across long contexts. This parameter sets the DRY sampling base value.",
      "dry_allowed_length": "DRY sampling reduces repetition in generated text even across long contexts. This parameter sets the allowed length for DRY sampling.",
      "dry_penalty_last_n": "DRY sampling reduces repetition in generated text even across long contexts. This parameter sets DRY penalty for the last n tokens.",
      "max_tokens": "The maximum number of token per output."
    }
  }
}
