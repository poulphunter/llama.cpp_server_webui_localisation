{
  "Header": {
    "tooltipSettings": "Impostazioni",
    "tooltipTheme": "Tema",
    "tooltipLanguage": "Lingua",
    "manualSettings": "Impostazioni manuali"
  },
  "ConversationList": {
    "Conversations": "Conversazioni",
    "sidebarClose": "chiudi barra laterale",
    "newConversation": "Nuova conversazione",
    "convInformation": "Le conversazioni vengono salvate nell'IndexedDB del browser",
    "deleteConfirm": "Sei sicuro di voler eliminare questa conversazione?",
    "downloadBtn": "Scarica",
    "deleteBtn": "Elimina",
    "conversationBtn": "Conversazioni",
    "closeBtn": "Vicino"
  },
  "ChatScreen": {
    "suggestions": "Ecco alcuni suggerimenti per te:",
    "sendMsgStart": "Invia un messaggio per iniziare",
    "textAreaPlaceHolder": "Digita un messaggio (Maiusc+Invio per aggiungere un nuovo line)",
    "stopBtn": "Stop",
    "sendBtn": "Invia"
  },
  "ChatMessage": {
    "Time": "Time",
    "Speed": "Velocità",
    "Tokens": "Token",
    "Cancel": "Annulla",
    "Submit": "Invia",
    "Edit": "✍\uFE0F Modifica",
    "Regenerate": "\uD83D\uDD04 Rigenera",
    "Thinking": "Pensieri",
    "ThoughtProcess": "Processo di pensiero",
    "ExtraContent": "Contenuto extra"
  },
  "MarkdownDisplay": {
    "copied": "Copiato!",
    "copy": "\uD83D\uDCCB Copia",
    "Run": "Esegui",
    "PythonInterpreter": "Interprete Python"
  },
  "Settings": {
    "Settings": "Impostazioni",
    "CloseBtn": "Vicino",
    "savedLocal": "Le impostazioni vengono salvate nel localStorage del browser",
    "resetConfirm": "Sei sicuro di voler ripristinare tutte le impostazioni?",
    "resetBtn": "Ripristina impostazioni predefinite",
    "saveBtn": "Salva",
    "loadPresetBtn": "Carica file JSON preset",
    "savePresetBtn": "Salva file JSON preset",
    "presetLabel": "Preset",
    "tooltipPresets": "Preset",
    "sections": {
      "General": "Generale",
      "Samplers": "Samplers",
      "Penalties": "Penalties",
      "Reasoning": "Ragionamento",
      "Advanced": "Avanzato",
      "Experimental": "Sperimentale"
    },
    "labels": {
      "apiKey": "Chiave API",
      "systemMessage": "Messaggio di sistema (sarà disabilitato se lasciato vuoto)",
      "samplers": "Coda dei campionatori",
      "temperature": "",
      "dynatemp_range": "",
      "dynatemp_exponent": "",
      "top_k": "",
      "top_p": "",
      "min_p": "",
      "xtc_probability": "",
      "xtc_threshold": "",
      "typical_p": "",
      "repeat_last_n": "",
      "repeat_penalty": "",
      "presence_penalty": "",
      "frequency_penalty": "",
      "dry_multiplier": "",
      "dry_base": "",
      "dry_allowed_length": "",
      "dry_penalty_last_n": "",
      "max_tokens": "",
      "customBtn": "(debug) Importa conversazione demo",
      "showThoughtInProgress": "Espandi tramite processo predefinito per generare messaggio",
      "excludeThoughtOnReq": "Escludi il processo di pensiero quando invii una richiesta all'API (consigliato per DeepSeek-R1)",
      "showTokensPerSecond": "Mostra token al secondo",
      "custom": "Configurazione JSON personalizzata (per maggiori informazioni, fai riferimento a",
      "customLinkLabel": "documentazione del server",
      "Experimental1": "Non è garantito che le funzionalità sperimentali funzionino correttamente.",
      "Experimental2": "Se riscontri problemi, crea un",
      "Experimental3": "segnala su Github. Specifica anche <b>webui/experimental</b> nel titolo del report e includi screenshot.",
      "Experimental4": "Alcune funzionalità potrebbero richiedere pacchetti scaricati da CDN, quindi necessitano di una connessione Internet.",
      "pyIntepreter1": "Abilita interprete Python",
      "pyIntepreter2": "Questa funzionalità utilizza",
      "pyIntepreter3": "scaricato da CDN. Per usare questa funzionalità, chiedi all'LLM di generare codice python all'interno di un blocco di codice markdown. Vedrai un pulsante \"Esegui\" sul blocco di codice, vicino al pulsante \"Copia\".",
      "handleSave1": "Valore per",
      "handleSave2": "deve essere una stringa",
      "handleSave3": "deve essere un numero",
      "handleSave4": "deve essere un booleano",
      "handleSave5": "deve essere un array"
    },
    "meaning": {
      "apiKey": "Imposta la chiave API se stai utilizzando l'opzione --api-key per il server.",
      "systemMessage": "Il messaggio di partenza che definisce come dovrebbe comportarsi il modello.",
      "samplers": "L'ordine in cui vengono applicati i campionatori, in modo semplificato. Il valore predefinito è 'dkypmxt': dry->top_k->typ_p->top_p->min_p->xtc->temperature",
      "temperature": "Controlla la casualità del testo generato influenzando la distribuzione di probabilità dei token di output. Più alto = più casuale, più basso = più focalizzato.",
      "dynatemp_range": "Componente aggiuntivo per il campionatore di temperatura. Il valore aggiunto all'intervallo di temperatura dinamica, che regola le probabilità in base all'entropia dei token.",
      "dynatemp_exponent": "Componente aggiuntivo per il campionatore di temperatura. Smussa la ridistribuzione delle probabilità in base al token più probabile.",
      "top_k": "Mantiene solo k token migliori.",
      "top_p": "Limita i token a quelli che insieme hanno una probabilità cumulativa di almeno p",
      "min_p": "Limita i token in base alla probabilità minima che un token venga preso in considerazione, rispetto alla probabilità del token più probabile.",
      "xtc_probability": "Il campionatore XTC taglia i token migliori; questo parametro controlla la possibilità di tagliare i token. 0 disabilita XTC.",
      "xtc_threshold": "Il campionatore XTC taglia i token migliori; questo parametro controlla la probabilità di token richiesta per tagliare quel token.",
      "typical_p": "Ordina e limita i token in base alla differenza tra probabilità logaritmica ed entropia.",
      "repeat_last_n": "Ultimi n token da considerare per penalizzare la ripetizione",
      "repeat_penalty": "Controlla la ripetizione delle sequenze di token nel testo generato",
      "presence_penalty": "Limita i token in base al fatto che appaiano o meno nell'output.",
      "frequency_penalty": "Limita i token in base alla frequenza con cui appaiano nell'output.",
      "dry_multiplier": "Il campionamento DRY riduce la ripetizione nel testo generato anche in contesti lunghi. Questo parametro imposta il moltiplicatore del campionamento DRY.",
      "dry_base": "Il campionamento DRY riduce la ripetizione nel testo generato anche in contesti lunghi. Questo parametro imposta il valore di base del campionamento DRY.",
      "dry_allowed_length": "Il campionamento DRY riduce la ripetizione nel testo generato anche in contesti lunghi. Questo parametro imposta la lunghezza consentita per il campionamento DRY.",
      "dry_penalty_last_n": "Il campionamento DRY riduce la ripetizione nel testo generato anche in contesti lunghi. Questo parametro imposta la penalità DRY per gli ultimi n token.",
      "max_tokens": "Il numero massimo di token per output."
    }
  }
}
